# network architecture
# encoder related
encoder: conformer
encoder_conf:
    output_size: 256    # dimension of attention
    attention_heads: 4
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 12      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: conv2d # encoder input type, you can chose conv2d, conv2d6 and conv2d8
    normalize_before: True
    cnn_module_kernel: 15
    activation_type: 'swish'
    pos_enc_layer_type: 'rel_pos'
    feature_norm : True

# decoder related
decoder: transformer
decoder_conf:
    attention_heads: 4
    linear_units: 2048
    num_blocks: 6
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.0
    src_attention_dropout_rate: 0.0

# hybrid CTC/attention
model_conf:
    ctc_weight: 0.3
    lsm_weight: 0.1     # label smoothing option
    length_normalized_loss: False

# feature extraction
collate_conf:
    # feature level config
    feature_extraction_conf:
        feature_type: 'fbank'
        mel_bins: 80
        frame_shift: 10
        frame_length: 25
        using_pitch: False
    feature_dither: 0.0 # add dither [-feature_dither,feature_dither] on fbank feature
    # data augmentation config
    use_speed_perturb: True
    use_spec_aug: True
    spec_aug_conf:
        warp_for_time: False
        num_t_mask: 2
        num_f_mask: 2
        prop_mask_t: 0.1
        prop_mask_f: 0.1
        max_t: 50
        max_f: 10
        max_w: 80
    use_dynamic_chunk: False
    use_dynamic_left_chunk: False
    decoding_chunk_size: 0
    static_chunk_size: 0
    num_decoding_left_chunks: -1

# dataset related
dataset_conf:
    max_length: 3000
    min_length: 0
    token_max_length: 30
    token_min_length: 1
    batch_type: 'bucket'    # bucket, static, dynamic
    frame_bucket_limit: '144, 204, 288, 400, 512, 600, 712, 800, 912, 1024, 1112, 1200, 1400, 1600, 2000, 3000'
    batch_bucket_limit: '40, 80, 80, 72, 72, 56, 56, 56, 40, 40, 40, 40, 24, 8, 8, 8'
    batch_factor: 1
    shuffle: True

# train option
grad_clip: 5
accum_grad: 1
max_epoch: 240
log_interval: 100

optim: adam
optim_conf:
    lr: 0.001
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 25000

cmvn_file: "/path/train/global_cmvn"
is_json_cmvn: True

exp_name: default
train_data: "/path/train/format.data"
eval_data: "/path/dev/format.data"
save_checkpoint: True
save_checkpoint_epochs: 1
save_checkpoint_steps: 460
keep_checkpoint_max: 30
save_checkpoint_path: "./"
device_target: "Ascend"
is_distributed: False
mixed_precision: True
resume_ckpt: ""
save_graphs: False
training_with_eval: True

# decode option
test_data: "/path/test/format.data"
dict: "/path/dict/lang_char.txt"
decode_ckpt: "avg_30.ckpt"
decode_mode: "attention" # ctc_greedy_search,ctc_prefix_beam_search,attention,attention_rescoring
full_graph: True
decode_batch_size: 1
ctc_weight: 0.0
beam_size: 10
penalty: 0.0

test_dataset_conf:
    max_length: 1200
    min_length: 0
    token_max_length: 30
    token_min_length: 1
    batch_type: 'bucket'    # bucket, static, dynamic
    frame_bucket_limit: '1200'
    batch_bucket_limit: '40'
    batch_factor: 1
    shuffle: False

# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: "obs://speech/corpus/aishell1"            # dataset path in OBS
train_url: "obs://speech/code/asr/workspace/"       # workspace path in OBS
checkpoint_url: ""                                  # pre-train ckpt in OBS
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
need_modelarts_dataset_unzip: False
modelarts_dataset_unzip_name: "corpora"
mnt_enable: False
ak: ""
sk: ""
server: ""
compile_url: ""

# mindinsight config
enable_profiling: False
enable_summary: True

# Config description for each option
use_dynamic_chunk:      # 'whether to use dynamic chunk or not, default: False'
use_dynamic_left_chunk: # 'whether to use dynamic left chunk for training, default: False'
decoding_chunk_size:    # 'ecoding chunk size for dynamic chunk, it is
                        # 0: default for training, use random dynamic chunk.
                        # <0: for decoding, use full chunk.
                        # >0: for decoding, use fixed chunk size as set.'
static_chunk_size:      # 'chunk size for static chunk training/decoding if it is greater than 0,
                        # if use_dynamic_chunk is true, this parameter will be ignored'
num_decoding_left_chunks: # 'number of left chunks, this is for decoding, the chunk size is decoding_chunk_size.
                          # >=0: use num_decoding_left_chunks
                          # <0: use all left chunks'

# infer for Ascend310
infer_model_path_1: ""
infer_model_path_2: ""
infer_data_path: ""
config_name: ""
label_file: "/path/test/text" # Used to calculate CER.
